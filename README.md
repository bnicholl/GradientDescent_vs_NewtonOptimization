# GradientDescent_vs_NewtonOptimization
Gradient descent and newton optimization are known as optimization algorithms. These algorithms are the heart, or more correctly worded, the brain of neural networks. These algo's enable neural nets to learn by using a functions derivatives in order to find the functions minimum. neural nets generaly find the minimum of an error. Here we will use a very popular function used by ML engineers to check the efficieny of our optimization algo. This function is known as the rosenbrock function. The journal I created is very visual, and so you should not need any calculus in order to follow along, although a basic understanding of rates of change would help in the understanding of this conent quite significantly.   
